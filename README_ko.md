# Simple Latent Diffusion Model

ì´ ì €ì¥ì†ŒëŠ” ê°„ë‹¨í•œ ì ì¬ í™•ì‚° ëª¨í˜•(Latent Diffusion Model)ì˜ êµ¬í˜„ì„ ì œê³µí•©ë‹ˆë‹¤. ì½”ë“œì™€ ë‚´ìš©ì€ ì§€ì†ì ìœ¼ë¡œ ê°±ì‹ ë  ì˜ˆì •ì…ë‹ˆë‹¤.

| ë°ì´í„° ì„¸íŠ¸                                | ì ì¬ ë³€ìˆ˜ì˜ ìƒì„± í”„ë¡œì„¸ìŠ¤               | ìƒì„±ëœ ë°ì´í„°                          |
|---------------------------------------------|-----------------------------------------|-----------------------------------------|
| Swiss-roll  | <img src="assets/swiss_roll.gif" width="300"/>   | <img src="assets/swiss_roll_image.png" width="300"/>  |
| CIFAR-10  | <img src="assets/cifar10.gif" width="300"/>   | <img src="assets/cifar10_image.png" width="300"/>  |
| CelebA  | <img src="assets/celeba.gif" width="300"/>   | <img src="assets/celeba_image.png" width="300"/>  |

## CLIPì„ ì´ìš©í•œ text-to-image ìƒì„±

ì•„ë˜ í…Œì´ë¸”ì€ CLIPì„ ì´ìš©í•œ text-to-image ê²°ê³¼ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë°ì´í„° ì„¸íŠ¸ëŠ” [í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ì˜ ê°€ìƒ ëª½íƒ€ì£¼ ë°ì´í„°](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=data&dataSetSn=618)ì…ë‹ˆë‹¤.

| **ì…ë ¥ í…ìŠ¤íŠ¸** | **ìƒì„±ëœ ì´ë¯¸ì§€** |
|----------------|------------------------------|
|ë™ê·¸ë€ ì–¼êµ´ì— í’ì„±í•˜ê³  ì‚´ì§ ê¸´ ì»¤íŠ¸ë¨¸ë¦¬ì™€ ê±°ì˜ ë‚˜ì˜¤ì§€ ì•Šì€ ì„±ëŒ€ê°€ ë‚¨ì„±ë³´ë‹¤ëŠ” ì—¬ì„±ì ì¸ ë¶„ìœ„ê¸°ë¥¼ ëƒ…ë‹ˆë‹¤. ë˜ë ·í•œ ëˆˆê³¼ ì…ìˆ ì´ ì¸ë¬¼ì˜ ì„¬ì„¸í•¨ì„ ë”ìš± ë¶€ê°ì‹œí‚¤ê³  ì§€ì ìœ¼ë¡œ ë³´ì´ê²Œ ë§Œë“­ë‹ˆë‹¤. | <img src="assets/Ex1.png" width="600"/> |
|í—¤ì–´ ì†ì§ˆì´ ë‹¤ì†Œ ë¯¸ìˆ™í•˜ì—¬ ì„¸ë ¨ëœ ëŠë‚Œì´ ë¶€ì¡±í•˜ë‹¤. ëˆˆ ëì´ ì˜¬ë¼ê°€ ìˆì–´ ëˆˆë¹›ì´ ë‚ ì¹´ë¡­ê³  ì˜ˆë¯¼í•´ ë³´ì¸ë‹¤. ì „ì²´ì ìœ¼ë¡œ ë§ˆë¥¸ ì²´ê²©ì¼ ê²ƒìœ¼ë¡œ ë³´ì´ë©°, ì—…ë¬´ ì²˜ë¦¬ ëŠ¥ë ¥ì€ ë›°ì–´ë‚˜ê² ì§€ë§Œ, êµìš° ê´€ê³„ëŠ” ì›ë§Œí•˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆë‹¤. | <img src="assets/Ex2.png" width="600"/> | 

### ë¼ì´ë¸Œ ë°ëª¨

í”„ë¡œì íŠ¸ë¥¼ ì§ì ‘ ì²´í—˜í•´ë³´ê³  ì‹¶ìœ¼ì‹ ê°€ìš”? ì•„ë˜ ë°°ì§€ë¥¼ í´ë¦­í•˜ì—¬ Hugging Face Spaceì—ì„œ ë°”ë¡œ ì‹¤í–‰í•´ ë³´ì„¸ìš”!

[![Hugging Face Spaces](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Spaces-blue)](https://juyeopdang-koface-ai.hf.space)

> **ì°¸ê³ :** Hugging Faceì˜ ë¬´ë£Œ í‹°ì–´(CPU)ë¡œ ìš´ì˜ë˜ê³  ìˆì–´, ì´ë¯¸ì§€ ìƒì„±ì— ì•½ 10ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## íŠœí† ë¦¬ì–¼

- [Tutorial for Latent Diffusion Model](notebook/simple_latent_diffusion_model_tutorial.ipynb)

## ì‚¬ìš©ë²•

ë‹¤ìŒ ì˜ˆì‹œëŠ” ì½”ë“œë¥¼ ì´ìš©í•˜ì—¬ ì–´ë–»ê²Œ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.

```python
import torch

from helper.painter import Painter
from helper.trainer import Trainer
from helper.data_generator import DataGenerator
from helper.loader import Loader
from helper.cond_encoder import CLIPEncoder

from auto_encoder.models.variational_auto_encoder import VariationalAutoEncoder
from clip.models.ko_clip import KoCLIPWrapper
from diffusion_model.sampler.ddim import DDIM
from diffusion_model.models.latent_diffusion_model import LatentDiffusionModel
from diffusion_model.network.unet import Unet
from diffusion_model.network.unet_wrapper import UnetWrapper

# Path to the configuration file
CONFIG_PATH = './configs/cifar10_config.yaml'

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Instantiate helper classes
painter = Painter()
loader = Loader()
data_generator = DataGenerator()

# Load CIFAR-10 dataset
data_loader = data_generator.cifar10(batch_size=128)

# Load CLIP model
clip = KoCLIPWrapper() # Any CLIP model from Hugging Face
cond_encoder = CLIPEncoder(clip, CONFIG_PATH) # Set encoder

# Train the Variational Autoencoder (VAE)
vae = VariationalAutoEncoder(CONFIG_PATH)  # Initialize the VAE model
trainer = Trainer(vae, vae.loss)  # Create a trainer for the VAE
trainer.train(dl=data_loader, epochs=100, file_name='vae', no_label=True)  # Train the VAE

# Train the Latent Diffusion Model (LDM)
sampler = DDIM(CONFIG_PATH)  # Initialize the DDIM sampler
network = UnetWrapper(Unet, CONFIG_PATH, cond_encoder)  # Initialize the U-Net network
ldm = LatentDiffusionModel(network, sampler, vae)  # Initialize the LDM
trainer = Trainer(ldm, ldm.loss)  # Create a trainer for the LDM
trainer.train(dl=data_loader, epochs=100, file_name='ldm', no_label=False)
# Train the LDM; set 'no_label=True' if the dataset does not include labels

# Load the trained models
vae = loader.model_load('models/VAE/vae', vae, is_ema=True)
ldm = loader.model_load('models/asian-composite-clip-ldm', ldm, is_ema=True)

# Generate samples using the trained diffusion model
ldm.eval()
ldm = ldm.to(device)
sample = ldm(n_samples=4, y = '...', gamma = 3)  # Generate 4 sample images, 'y' represents any conditions, 'gamma' means guidance scale
painter.show_images(sample)  # Display the generated images
```

## ì°¸ê³ 
- [lucidrains/denoising-diffusion-pytorch](https://github.com/lucidrains/denoising-diffusion-pytorch)
- [CompVis/latent-diffusion](https://github.com/CompVis/latent-diffusion)
- [labmlai/annotated_deep_learning_paper_implementations](https://github.com/labmlai/annotated_deep_learning_paper_implementations/tree/master/labml_nn/diffusion/stable_diffusion)
